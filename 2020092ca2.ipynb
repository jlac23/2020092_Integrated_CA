{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bac2e33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e9f01a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1fa2adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccb16f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local[*]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f90e6a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2f2593e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.                                                       \n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.csv('/ca2/ProjectTweets.csv',header=True,sep=\"|\")\n",
    "\n",
    "print(df.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c76eefcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark is from the previous example.\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# A CSV dataset is pointed to by path.\n",
    "\n",
    "path = \"/ca2/ProjectTweets.csv\"\n",
    "TweetsDF = spark.read.csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "daa1bbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TweetsDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5c325ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a temporary view using the DataFrame\n",
    "TweetsDF.createOrReplaceTempView(\"tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "331690b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|Row|       IDs|                Date|   Query|           User|               Tweet|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|  0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|  1|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|  2|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|  3|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|  4|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "|  5|1467811372|Mon Apr 06 22:20:...|NO_QUERY|       joy_wolf|@Kwesidei not the...|\n",
      "|  6|1467811592|Mon Apr 06 22:20:...|NO_QUERY|        mybirch|         Need a hug |\n",
      "|  7|1467811594|Mon Apr 06 22:20:...|NO_QUERY|           coZZ|@LOLTrish hey  lo...|\n",
      "|  8|1467811795|Mon Apr 06 22:20:...|NO_QUERY|2Hood4Hollywood|@Tatiana_K nope t...|\n",
      "|  9|1467812025|Mon Apr 06 22:20:...|NO_QUERY|        mimismo|@twittera que me ...|\n",
      "| 10|1467812416|Mon Apr 06 22:20:...|NO_QUERY| erinx3leannexo|spring break in p...|\n",
      "| 11|1467812579|Mon Apr 06 22:20:...|NO_QUERY|   pardonlauren|I just re-pierced...|\n",
      "| 12|1467812723|Mon Apr 06 22:20:...|NO_QUERY|           TLeC|@caregiving I cou...|\n",
      "| 13|1467812771|Mon Apr 06 22:20:...|NO_QUERY|robrobbierobert|@octolinz16 It it...|\n",
      "| 14|1467812784|Mon Apr 06 22:20:...|NO_QUERY|    bayofwolves|@smarrison i woul...|\n",
      "| 15|1467812799|Mon Apr 06 22:20:...|NO_QUERY|     HairByJess|@iamjazzyfizzle I...|\n",
      "| 16|1467812964|Mon Apr 06 22:20:...|NO_QUERY| lovesongwriter|Hollis' death sce...|\n",
      "| 17|1467813137|Mon Apr 06 22:20:...|NO_QUERY|       armotley|about to file taxes |\n",
      "| 18|1467813579|Mon Apr 06 22:20:...|NO_QUERY|     starkissed|@LettyA ahh ive a...|\n",
      "| 19|1467813782|Mon Apr 06 22:20:...|NO_QUERY|      gi_gi_bee|@FakerPattyPattz ...|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL statements can be run by using the sql methods provided by spark\n",
    "tweetsDF = spark.sql(\"SELECT _c0 as Row, _c1 as IDs, _c2 as Date, _c3 as Query, _c4 as User, _c5 as Tweet FROM tweets\")\n",
    "tweetsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cb0e71bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#df = spark.read.load(\"examples/src/main/resources/users.parquet\")\n",
    "tweetsDF.select(\"Tweet\").write.save(\"Tweet.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b327e5c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'location' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18807/435111923.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlocation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'location' is not defined"
     ]
    }
   ],
   "source": [
    "location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6efc17d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|           User|\n",
      "+---------------+\n",
      "|_TheSpecialOne_|\n",
      "|  scotthamilton|\n",
      "|       mattycus|\n",
      "|        ElleCTF|\n",
      "|         Karoli|\n",
      "|       joy_wolf|\n",
      "|        mybirch|\n",
      "|           coZZ|\n",
      "|2Hood4Hollywood|\n",
      "|        mimismo|\n",
      "| erinx3leannexo|\n",
      "|   pardonlauren|\n",
      "|           TLeC|\n",
      "|robrobbierobert|\n",
      "|    bayofwolves|\n",
      "|     HairByJess|\n",
      "| lovesongwriter|\n",
      "|       armotley|\n",
      "|     starkissed|\n",
      "|      gi_gi_bee|\n",
      "+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "userDF = spark.sql(\"SELECT _c4 as User FROM tweets\")\n",
    "userDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5cfbde51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              Tweets|\n",
      "+--------------------+\n",
      "|@switchfoot http:...|\n",
      "|is upset that he ...|\n",
      "|@Kenichan I dived...|\n",
      "|my whole body fee...|\n",
      "|@nationwideclass ...|\n",
      "|@Kwesidei not the...|\n",
      "|         Need a hug |\n",
      "|@LOLTrish hey  lo...|\n",
      "|@Tatiana_K nope t...|\n",
      "|@twittera que me ...|\n",
      "|spring break in p...|\n",
      "|I just re-pierced...|\n",
      "|@caregiving I cou...|\n",
      "|@octolinz16 It it...|\n",
      "|@smarrison i woul...|\n",
      "|@iamjazzyfizzle I...|\n",
      "|Hollis' death sce...|\n",
      "|about to file taxes |\n",
      "|@LettyA ahh ive a...|\n",
      "|@FakerPattyPattz ...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textDF = spark.sql(\"SELECT _c5 as Tweets FROM tweets\")\n",
    "textDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0f335508",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of rows: 1600000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:==================================================>   (188 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of distinct rows: 1581466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 25:====================================================> (196 + 1) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Looking for duplicates\n",
    "print('Count of rows: {0}'.format(textDF.count()))\n",
    "print('Count of distinct rows: {0}'.format(textDF.distinct().count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ba967a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, DateType, LongType, TimestampType, StringType, FloatType, DoubleType, DecimalType, StructType, StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "abca836a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-05 00:47:53,234 WARN impl.BlockReaderFactory: I/O error constructing remote block reader.\n",
      "java.nio.channels.ClosedByInterruptException\n",
      "\tat java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)\n",
      "\tat sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:658)\n",
      "\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2939)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:821)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:746)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:379)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:644)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:575)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1496)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:728)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:766)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:829)\n",
      "\tat java.io.DataInputStream.read(DataInputStream.java:149)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:218)\n",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:176)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n",
      "\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:255)\n",
      "\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)\n",
      "\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:312)\n",
      "\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:243)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:621)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\n"
     ]
    }
   ],
   "source": [
    "tweet = sc.textFile('/ca2/ProjectTweets.csv')\n",
    "header = tweet.first()\n",
    "\n",
    "tweet = tweet \\\n",
    "    .filter(lambda row: row != header) \\\n",
    "    .map(lambda row: [int(elem) for elem in row.split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "08b23d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = StructType([\n",
    "    StructField(\"row\", IntegerType(),True),\n",
    "    StructField(\"id\", IntegerType(),True),\n",
    "    StructField(\"date\", DateType(),True),\n",
    "    StructField(\"query\", StringType(),True),\n",
    "    StructField(\"user\", StringType(),True),\n",
    "    StructField(\"text\", StringType(),True) \n",
    "    \n",
    "])\n",
    "schema = typ.StructType(pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ec3d7f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = spark.read.format('csv') \\\n",
    ".schema(pt) \\\n",
    ".load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b1274673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[row: int, id: int, date: date, query: string, user: string, text: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display (tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "43b9b88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_df = spark.createDataFrame(tweet,pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4239c6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- row: integer (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- query: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fraud_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "868c1dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- row: integer (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- query: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ac67e545",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'QueryDF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18807/4193415172.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mQueryDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'QueryDF' is not defined"
     ]
    }
   ],
   "source": [
    "QueryDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fcb8768a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|           User|\n",
      "+---------------+\n",
      "|_TheSpecialOne_|\n",
      "|  scotthamilton|\n",
      "|       mattycus|\n",
      "|        ElleCTF|\n",
      "|         Karoli|\n",
      "|       joy_wolf|\n",
      "|        mybirch|\n",
      "|           coZZ|\n",
      "|2Hood4Hollywood|\n",
      "|        mimismo|\n",
      "| erinx3leannexo|\n",
      "|   pardonlauren|\n",
      "|           TLeC|\n",
      "|robrobbierobert|\n",
      "|    bayofwolves|\n",
      "|     HairByJess|\n",
      "| lovesongwriter|\n",
      "|       armotley|\n",
      "|     starkissed|\n",
      "|      gi_gi_bee|\n",
      "+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "userDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f24ab04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
